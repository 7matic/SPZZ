{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ‚ùó GENERAL INSTRUCTIONS ‚ùó\n",
        "- You should be running this notebook using Google Colab.\n",
        "- Make sure to change your runtime type by pressing 'Runtime' -> 'Change runtime type' and use one of the available GPUs instead of using the CPU.\n",
        "---\n",
        "- Before uploading, compress the `Google_Colab` folder into a zip file on your local machine.\n",
        "- In Google Colab, use the file upload feature to upload the zipped folder.\n",
        "After uploading, unzip the folder in Colab using the code block below.\n",
        "- After unzipping, the `Google_Colab` folder should have the following structure:\n",
        "```\n",
        "Google_Colab/\n",
        "‚îÇ\n",
        "‚îú‚îÄ‚îÄ data/\n",
        "‚îÇ ‚îú‚îÄ‚îÄ training/\n",
        "‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ train_data.json\n",
        "‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ ...\n",
        "```\n",
        "---\n",
        "- Additional relevant information and instructions for training a custom NER (Named Entity Recognition) model will be provided throughout the remaining code blocks."
      ],
      "metadata": {
        "id": "oWOi36KHZvCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/Google_Colab.zip"
      ],
      "metadata": {
        "id": "QZYkMfoFvyst"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üíª CODE"
      ],
      "metadata": {
        "id": "G7x_UJpPdQD1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GnBAk5UZDxO",
        "outputId": "ef76812e-d5b0-46e8-c53f-90e85d90514d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spacy_transformers\n",
            "  Downloading spacy_transformers-1.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (197 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m197.8/197.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<4.0.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from spacy_transformers) (3.6.1)\n",
            "Requirement already satisfied: transformers<4.36.0,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy_transformers) (4.35.2)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from spacy_transformers) (2.1.0+cu121)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy_transformers) (2.4.8)\n",
            "Collecting spacy-alignments<1.0.0,>=0.7.2 (from spacy_transformers)\n",
            "  Downloading spacy_alignments-0.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (313 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m314.0/314.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy_transformers) (1.23.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (1.1.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (3.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy_transformers) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy_transformers) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy_transformers) (3.2.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy_transformers) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy_transformers) (2.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers<4.36.0,>=3.4.0->spacy_transformers) (0.19.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.36.0,>=3.4.0->spacy_transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<4.36.0,>=3.4.0->spacy_transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<4.36.0,>=3.4.0->spacy_transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.36.0,>=3.4.0->spacy_transformers) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.5.0->spacy_transformers) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.5.0->spacy_transformers) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<4.0.0,>=3.5.0->spacy_transformers) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->spacy_transformers) (1.3.0)\n",
            "Installing collected packages: spacy-alignments, spacy_transformers\n",
            "Successfully installed spacy-alignments-0.9.1 spacy_transformers-1.3.3\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Collecting weasel<0.4.0,>=0.1.0 (from spacy)\n",
            "  Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy)\n",
            "  Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
            "Installing collected packages: cloudpathlib, weasel, spacy\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.6.1\n",
            "    Uninstalling spacy-3.6.1:\n",
            "      Successfully uninstalled spacy-3.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.6.0 requires spacy<3.7.0,>=3.6.0, but you have spacy 3.7.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cloudpathlib-0.16.0 spacy-3.7.2 weasel-0.3.4\n"
          ]
        }
      ],
      "source": [
        "# spacy_transformers provides spaCy components and architectures for using transformer models, such as BERT, with spaCy.\n",
        "!pip install spacy_transformers\n",
        "# Upgrade the spacy package to the latest version to ensure compatibility with recent features and improvements in natural language processing.\n",
        "!pip install -U spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_n_3NdI9ZLjm"
      },
      "outputs": [],
      "source": [
        "# tokens: A submodule in spaCy that deals with tokenization. Tokenization is the process of splitting text into words, phrases, symbols, or other meaningful elements called tokens.\n",
        "# DocBin: class from spaCy to serialize and deserialize collections of Doc objects efficiently.\n",
        "from spacy.tokens import DocBin\n",
        "# Library that provides a fast, extensible progress bar.\n",
        "from tqdm import tqdm\n",
        "import spacy\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_rAlJm9JcpQW"
      },
      "outputs": [],
      "source": [
        "cv_data = json.load(open('/content/Google_Colab/data/training/train_data.json'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YMdRDZ2dOaA",
        "outputId": "1977b43b-5725-453b-805c-0b8504289401"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-15 23:27:03.753487: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-15 23:27:03.753538: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-15 23:27:03.755078: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-15 23:27:05.219545: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;2m‚úî Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m‚úî Saved config\u001b[0m\n",
            "config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
          ]
        }
      ],
      "source": [
        "# This helps in setting up a spaCy training configuration with appropriate defaults based on given setup and requirements.\n",
        "# It will read the base_config.cfg, fill in any missing values with defaults, and then save the complete configuration to config.cfg.\n",
        "!python -m spacy init fill-config /content/Google_Colab/data/training/base_config.cfg config.cfg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QJLo-m6Ud8L2"
      },
      "outputs": [],
      "source": [
        "# The function processes training data for NER by creating spaCy Doc objects with their respective entity annotations,\n",
        "# handles potential errors, and stores these Doc objects in a DocBin for efficient batch processing.\n",
        "def get_spacy_doc(file, data):\n",
        "\n",
        "  # Create a new spaCy pipeline for English without any NLP components.\n",
        "  nlp = spacy.blank(\"en\")\n",
        "  # Initialize a DocBin object for efficient storage of spaCy Doc objects.\n",
        "  db = DocBin()\n",
        "\n",
        "  for text, annot in tqdm(data):\n",
        "\n",
        "     # Create a Doc object from the text. This only tokenizes the text.\n",
        "    doc = nlp.make_doc(text)\n",
        "\n",
        "    # Extract the entity annotations for the current text.\n",
        "    annot = annot['entities']\n",
        "\n",
        "    # Initialize a list to store the entity spans.\n",
        "    ents = []\n",
        "\n",
        "    # Keep track of character indices already part of an entity.\n",
        "    entity_indices = []\n",
        "\n",
        "    # start: The starting index (position) of an entity in the text.\n",
        "    # end: The ending index of the entity in the text.\n",
        "    # label: The label/classification of the entity (e.g., ORG for organization, PERSON for person's name).\n",
        "    for start, end, label in annot:\n",
        "      # Check if the current span overlaps with previous entities.\n",
        "      skip_entity = False\n",
        "      for idx in range(start, end):\n",
        "        if idx in entity_indices:\n",
        "          skip_entity = True\n",
        "          break\n",
        "      # Skip the current entity if it overlaps.\n",
        "      # Overlapping entities can confuse the model and are generally not allowed in most NER systems.\n",
        "      if skip_entity == True:\n",
        "        continue\n",
        "\n",
        "      entity_indices = entity_indices + list(range(start, end))\n",
        "\n",
        "      # Try to create a character span for the entity.\n",
        "      # Creating spans is crucial because it directly maps the annotated labels (like \"PERSON\", \"ORGANIZATION\") to specific segments of text.\n",
        "      # This mapping is where the NER model learns from.\n",
        "      try:\n",
        "        span = doc.char_span(start, end, label=label, alignment_mode='strict')\n",
        "      except:\n",
        "        continue\n",
        "\n",
        "      # Handle cases where the span is None (invalid span).\n",
        "      if span is None:\n",
        "        # Write error data to the file.\n",
        "        err_data = str([start, end]) + \"   \" + str(text) + \"\\n\"\n",
        "        file.write(err_data)\n",
        "      else:\n",
        "        ents.append(span)\n",
        "\n",
        "    # Try to set the entities for the Doc and add it to the DocBin.\n",
        "    try:\n",
        "      doc.ents = ents\n",
        "      db.add(doc)\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "  # Return the filled DocBin object.\n",
        "  return db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6rnPory2jYOp"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# The purpose of splitting data into training and test sets is to evaluate the performance of model on unseen data.\n",
        "# The model is trained on the training set and then tested on the test set.\n",
        "# 70% of the data is allocated to the train set, consequently, the remaining 30% represents the test set.\n",
        "train, test = train_test_split(cv_data, test_size=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIuHoCsJjsoZ",
        "outputId": "ae22127c-ae4a-4d0b-d20b-74d23540346b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140/140 [00:01<00:00, 95.26it/s] \n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:00<00:00, 90.58it/s] \n"
          ]
        }
      ],
      "source": [
        "# Processes training and test data, converts them to a format that spaCy can use for training a model,\n",
        "# and logs any errors that occur during processing to a text file.\n",
        "\n",
        "file = open('error.txt', 'w')\n",
        "\n",
        "db = get_spacy_doc(file, train)\n",
        "db.to_disk('train_data.spacy')\n",
        "\n",
        "db = get_spacy_doc(file, test)\n",
        "db.to_disk('test_data.spacy')\n",
        "\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGdN5V1glFYM",
        "outputId": "d987f51e-42fb-467f-c6e6-5b9685cdbd87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-15 23:27:45.115653: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-15 23:27:45.115706: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-15 23:27:45.116885: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-15 23:27:46.156796: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;2m‚úî Created output directory: output\u001b[0m\n",
            "\u001b[38;5;4m‚Ñπ Saving to output directory: output\u001b[0m\n",
            "\u001b[38;5;4m‚Ñπ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "config.json: 100% 481/481 [00:00<00:00, 2.52MB/s]\n",
            "vocab.json: 100% 899k/899k [00:00<00:00, 18.6MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 33.9MB/s]\n",
            "tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 57.4MB/s]\n",
            "model.safetensors: 100% 499M/499M [00:04<00:00, 103MB/s] \n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[38;5;2m‚úî Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4m‚Ñπ Pipeline: ['transformer', 'ner']\u001b[0m\n",
            "\u001b[38;5;4m‚Ñπ Initial learn rate: 0.0\u001b[0m\n",
            "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  -------------  --------  ------  ------  ------  ------\n",
            "  0       0        2922.51   1592.37    0.30    0.15    5.71    0.00\n",
            "  4     200      226809.61  65647.91   27.09   36.65   21.49    0.27\n",
            "  8     400        5690.39  21144.66   53.52   49.47   58.29    0.54\n",
            " 12     600       13757.78  18807.72   50.53   55.80   46.17    0.51\n",
            "\n",
            "\u001b[31mAborted.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# This command starts the training process for a spaCy model using the specified configuration file and data.\n",
        "# The trained model and other related files will be saved in the ./output directory.\n",
        "!python -m spacy train ./config.cfg --output ./output --paths.train ./train_data.spacy --paths.dev ./test_data.spacy --gpu-id 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save your trained spaCy model to Google Drive so you can then download it for local use.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "DbSFtr_1pJ1i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbe06980-13b8-4110-d729-6965e2243d08"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "current_time = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "model_name = f\"model-best-{current_time}.zip\"\n",
        "\n",
        "# Zip the model.\n",
        "!zip -r \"/content/{model_name}\" /content/output/model-best"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gdu2LZv95e_L",
        "outputId": "9948309a-f288-42e3-e35f-57250aeaf5e0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/output/model-best/ (stored 0%)\n",
            "  adding: content/output/model-best/transformer/ (stored 0%)\n",
            "  adding: content/output/model-best/transformer/cfg (stored 0%)\n",
            "  adding: content/output/model-best/transformer/model (deflated 13%)\n",
            "  adding: content/output/model-best/ner/ (stored 0%)\n",
            "  adding: content/output/model-best/ner/cfg (deflated 33%)\n",
            "  adding: content/output/model-best/ner/moves (deflated 73%)\n",
            "  adding: content/output/model-best/ner/model (deflated 8%)\n",
            "  adding: content/output/model-best/config.cfg (deflated 61%)\n",
            "  adding: content/output/model-best/meta.json (deflated 65%)\n",
            "  adding: content/output/model-best/tokenizer (deflated 81%)\n",
            "  adding: content/output/model-best/vocab/ (stored 0%)\n",
            "  adding: content/output/model-best/vocab/vectors (deflated 45%)\n",
            "  adding: content/output/model-best/vocab/vectors.cfg (stored 0%)\n",
            "  adding: content/output/model-best/vocab/lookups.bin (stored 0%)\n",
            "  adding: content/output/model-best/vocab/key2row (stored 0%)\n",
            "  adding: content/output/model-best/vocab/strings.json (deflated 76%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy the zipped model to Google Drive.\n",
        "!cp \"/content/{model_name}\" /content/drive/MyDrive/\n",
        "\n",
        "# When you download your zipped model, you only have to use the 'model-best' directory and replace existing one used locally with the new one."
      ],
      "metadata": {
        "id": "PNW4l57z54Id"
      },
      "execution_count": 12,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}